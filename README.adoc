= Oparaca reimplemented in Rust
:toc:
:toc-placement: preamble
:toclevels: 2

// Need some preamble to get TOC:
{empty}

== Introduction
We reimplemented https://github.com/hpcclab/OaaS[Oparaca] in Rust. The original Oparaca was implemented in Java. 

== Build this project
=== Prerequisites 

==== Build
- rust
+
https://www.rust-lang.org/tools/install[]

- protobuf-compiler
+
`sudo apt install -y protobuf-compiler`

- libssl-dev
+
`sudo apt install libssl-dev`

==== Development

- container runtime (Docker or Podman)
- https://github.com/kubernetes/kubectl[kubectl]
- https://github.com/kubernetes-sigs/kind[kind]
- https://github.com/casey/just[just]

=== Build binary
[source,bash]
----
cargo build -r
----

=== Build container image
[source,bash]
----
docker compose -f docker-compose.release.yml build
----

NOTE: You can skip building the binary and run the above command to build the container image.

== High-level Architecture Overview

This repository is an Object-as-a-Service platform reimplementation with a Rust data plane and Rust-native control-plane services. It follows a clear separation of concerns:

- Control Plane: defines desired state (classes/functions, packages), orchestrates deployments, and exposes APIs to users and automation.
- Data Plane: executes function runtimes and hosts the Object Data Grid Manager (ODGM) for low-latency object access and coordination.

.OaaS-RS System Architecture
[mermaid]
ifdef::env-github[[source,mermaid]]
....
graph TB
    subgraph "üåê External Clients"
        CLI["üñ•Ô∏è OaaS CLI (oprc-cli)"]
        API["üì° REST/gRPC Clients"]
    end
    
    subgraph "üìä Control Plane (Management Layer)"
        PM["üì¶ Package Manager (PM)<br/>‚Ä¢ Package Registry<br/>‚Ä¢ Multi-Cluster Orchestration<br/>‚Ä¢ Deployment Distribution"]
        CRM1["‚öôÔ∏è Class Runtime Manager (CRM-1)<br/>‚Ä¢ Kubernetes Controller<br/>‚Ä¢ ClassRuntime CRD<br/>‚Ä¢ NFR Enforcement"]
        CRM2["‚öôÔ∏è Class Runtime Manager (CRM-N)<br/>‚Ä¢ Kubernetes Controller<br/>‚Ä¢ ClassRuntime CRD<br/>‚Ä¢ NFR Enforcement"]
    end
    
    subgraph "‚ö° Data Plane Cluster 1"
        GW1["üö™ Gateway<br/>‚Ä¢ REST/gRPC Ingress<br/>‚Ä¢ Protocol Translation<br/>‚Ä¢ Request Routing"]
        ROUTER1["üîÄ Router<br/>‚Ä¢ Zenoh Messaging<br/>‚Ä¢ Pub/Sub & RPC<br/>‚Ä¢ Service Discovery"]
        ODGM1["üóÑÔ∏è ODGM<br/>‚Ä¢ Object Data Grid<br/>‚Ä¢ Stateful Storage<br/>‚Ä¢ Collection Management"]
        FUNC1["üîß Function Runtimes<br/>‚Ä¢ Serverless Execution<br/>‚Ä¢ Auto-scaling<br/>‚Ä¢ Multi-language Support"]
    end
    
    subgraph "‚ö° Data Plane Cluster N"
        GW2["üö™ Gateway<br/>‚Ä¢ REST/gRPC Ingress<br/>‚Ä¢ Protocol Translation<br/>‚Ä¢ Request Routing"]
        ROUTER2["üîÄ Router<br/>‚Ä¢ Zenoh Messaging<br/>‚Ä¢ Pub/Sub & RPC<br/>‚Ä¢ Service Discovery"]
        ODGM2["üóÑÔ∏è ODGM<br/>‚Ä¢ Object Data Grid<br/>‚Ä¢ Stateful Storage<br/>‚Ä¢ Collection Management"]
        FUNC2["üîß Function Runtimes<br/>‚Ä¢ Serverless Execution<br/>‚Ä¢ Auto-scaling<br/>‚Ä¢ Multi-language Support"]
    end
    
    subgraph "üèóÔ∏è Infrastructure Layer"
        subgraph K8S1 ["‚ò∏Ô∏è Kubernetes Cluster 1"]
            K8S1_CORE["üîß Container Orchestration<br/>‚Ä¢ Resource Management<br/>‚Ä¢ Service Mesh"]
            PROM1["üìà Prometheus<br/>‚Ä¢ Metrics Collection<br/>‚Ä¢ NFR Monitoring<br/>‚Ä¢ Performance Analytics"]
        end
        
        subgraph K8S2 ["‚ò∏Ô∏è Kubernetes Cluster N"]
            K8S2_CORE["üîß Container Orchestration<br/>‚Ä¢ Resource Management<br/>‚Ä¢ Service Mesh"]
            PROM2["üìà Prometheus<br/>‚Ä¢ Metrics Collection<br/>‚Ä¢ NFR Monitoring<br/>‚Ä¢ Performance Analytics"]
        end
    end
    
    %% Control Plane Connections
    CLI --> PM
    API --> PM
    PM -->|gRPC Deploy/Status/Delete| CRM1
    PM -->|gRPC Deploy/Status/Delete| CRM2
    
    %% Data Plane Connections
    CLI --> ROUTER1
    CLI --> ROUTER2
    API --> GW1
    API --> GW2
    GW1 --> ROUTER1
    GW2 --> ROUTER2
    ROUTER1 -.->|Zenoh Mesh| ROUTER2
    ROUTER1 --> ODGM1
    ROUTER1 --> FUNC1
    ROUTER2 --> ODGM2
    ROUTER2 --> FUNC2
    ODGM1 -.->|Cross-Cluster Replication| ODGM2
    
    %% Infrastructure Connections
    CRM1 -.->|Kubernetes API| K8S1_CORE
    CRM2 -.->|Kubernetes API| K8S2_CORE
    CRM1 -.->|Metrics Query| PROM1
    CRM2 -.->|Metrics Query| PROM2
    PROM1 -.->|Scrape Metrics| ODGM1
    PROM1 -.->|Scrape Metrics| FUNC1
    PROM2 -.->|Scrape Metrics| ODGM2
    PROM2 -.->|Scrape Metrics| FUNC2
    
    %% Styling
    classDef controlPlane fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef dataPlane fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef infrastructure fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef external fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class PM,CRM1,CRM2 controlPlane
    class GW1,GW2,ROUTER1,ROUTER2,ODGM1,ODGM2,FUNC1,FUNC2 dataPlane
    class K8S1_CORE,K8S2_CORE,PROM1,PROM2 infrastructure
    class CLI,API external
....

=== Core Components

* Package Manager (PM) ‚Äî control-plane
** Location: link:control-plane/oprc-pm/[control-plane/oprc-pm/]
** Role: Front-door for class/package operations, talking to CRM (gRPC). Can be extended to multi-env coordination.
** Docs: link:control-plane/oprc-pm/README.md[PM README]

* Class Runtime Manager (CRM) ‚Äî control-plane
** Location: link:control-plane/oprc-crm/[control-plane/oprc-crm/]
** Role: Kubernetes-native controller that manages the lifecycle of Class deployments via a CRD called `ClassRuntime`.
** API: gRPC (using `commons/oprc-grpc` protobufs) for Deploy/Status/Delete.
** Behavior: Reads CRDs and applies Kubernetes resources (Deployments/Services or Knative) via server-side apply.
** Docs: link:control-plane/oprc-crm/README.md[CRM README]


* ODGM (Object Data Grid Manager) ‚Äî data-plane
** Location: link:data-plane/oprc-odgm/[data-plane/oprc-odgm/]
** Role: High-performance object data grid, replacing the original Invoker; provides stateful, low-latency data services for classes.
** Deployment: Runs as a separate Kubernetes Deployment/Service per Class (not a sidecar). Supports collections and abstracted replication.
** Docs: link:data-plane/oprc-odgm/README.adoc[ODGM README]

* Gateway ‚Äî data-plane
** Location: link:data-plane/oprc-gateway/[data-plane/oprc-gateway/]
** Role: External ingress point (REST/gRPC) for clients, translating API calls into internal invocations.

* Router ‚Äî data-plane
** Location: link:data-plane/oprc-router/[data-plane/oprc-router/]
** Role: Message routing built on Zenoh; supports pub/sub and ZRPC patterns between components.

* Commons ‚Äî shared libraries
** Location: link:commons/[commons/]
** Includes: models (`oprc-models`), protobuf/gRPC (`oprc-grpc`), Zenoh integration (`oprc-zenoh`), configuration helpers, storage abstractions.


=== Communication and Protocols

* External API: gRPC (and REST via Gateway) for package/class lifecycle operations.
* CRM API: gRPC DeploymentService (Deploy, GetDeploymentStatus, DeleteDeployment) defined in `commons/oprc-grpc`.
* Internal Messaging: Zenoh for pub/sub and `flare-zrpc` for request/response on top of Zenoh.
* Kubernetes API: CRM uses kube-rs with server-side apply to manage cluster state.

=== Data and Configuration Models

* ClassRuntime CRD (CRM): source of truth for one Class deployment per record. Contains function specs, NFR requirements, template hint, addons (simple list), and ODGM collections configuration.
* OaaS Models (commons/oprc-models): defines domain types used across services (deployments, NFRs, runtime state, packages).

Docs: link:control-plane/oprc-crm/README.md[CRM], link:control-plane/oprc-pm/README.md[PM]

=== Typical Flows

1. Deploy
** PM receives a deploy request from a user or CI/CD and calls CRM‚Äôs gRPC Deploy.
** CRM upserts a ClassRuntime CRD, adds finalizer, and enqueues reconcile.
** Reconcile selects a template (Dev/Edge/Cloud) based on template_hint ‚Üí NFR heuristics ‚Üí profile default, composes resources (function Deployments/Services and an ODGM Deployment/Service), and applies them via SSA.
** Function pods receive env/config to discover ODGM (service address and collections).

2. Status
** PM calls CRM GetDeploymentStatus.
** CRM maps CRD conditions to a structured status (Available/Progressing/Degraded/Unknown) and returns resource references.

3. Delete
** PM calls CRM DeleteDeployment.
** CRM marks for deletion; controller removes function and ODGM resources, then clears finalizer.

=== Templates and Addons

* Templates: The TemplateManager acts as a registry of templates (e.g., Dev, Edge, Cloud; extensible). Each template emits Kubernetes resources tailored for its environment (replicas, HPA, or Knative).
* Selection: Order of precedence ‚Äî CRD spec.template_hint ‚Üí NFR heuristics (e.g., min_throughput_rps, max_latency_ms, availability) ‚Üí CRM profile default.
* Addons: Simple list on the CRD (e.g., ["odgm"]). ODGM is currently mandatory in practice and renders as a separate Deployment/Service per Class.
* Injection: When addons are enabled, CRM injects discovery/config (env or ConfigMap) into function pods (e.g., `ODGM_SERVICE`, `ODGM_COLLECTION`).
** `ODGM_COLLECTION` now carries a JSON array of complete CreateCollectionRequest objects (one per logical collection name) rather than just names.
** Partition and replica counts plus shard_type are supplied by PM (partition_count, replica_count, shard_type fields on the deployment spec‚Äôs ODGM data section) and are propagated verbatim; CRM does not auto-scale partitions yet (see below).

=== ODGM Partitioning & Replication Strategy (Current Plan)

* Partition Count: Provided by PM based on throughput / parallelism heuristics. Partitions are stable identifiers embedded in object keys. CRM will surface recommendations in status (future) but will not mutate partition_count automatically to avoid rehash churn.
* Replica Count: Also set by PM from availability targets (NFRs). CRM may in the future suggest increased replicas when enforcement detects sustained utilization gaps, but authoritative changes remain PM-driven for deterministic rollouts.
* Shard Type: Defaults to `mst` (multi-version state tree) for dev/edge; PM can request alternative (e.g., `raft`) for stronger consistency. CRM passes through unchanged.
* Collections: Each logical collection name expands into a minimal CreateCollectionRequest with uniform partition/replica/shard parameters. Additional per-collection overrides are deliberately deferred to keep early API surface minimal.
* Enforcement Roadmap:
** Phase 1 (now): Pass-through; validate shape; surface in env var.
** Phase 2: Observe metrics (per-partition load, replica health) and emit recommendations in ClassRuntime status (non-disruptive).
** Phase 3: Optional enforcement mode where CRM can request PM to re-deploy with adjusted replica_count (never unilateral changes) or trigger a partition expansion workflow.
** Partition Expansion: Will require two-phase migration (allocate new partitions, shadow replicate, cut-over).
** See also: link:docs/NFR_ENFORCEMENT_DESIGN.md[NFR Enforcement Design]

=== Observability and Operations

* Tracing: consistent correlation IDs propagated via gRPC metadata and recorded on CRDs.
* Metrics: reconciliation timings and errors; Prometheus integration planned.
* Reliability: idempotent operations (by deployment_id), deadline-aware gRPC handling, conservative backoff, and eventual leader election.

